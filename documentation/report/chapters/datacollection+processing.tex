\section*{Time Series data}
Different sources
Price categories: Retail prices, wholesale prices

\subsection*{Wholesale prices}

\subsubsection*{Wholesale price index}
(taken from investopedia.com)\\
An index that measures and tracks the changes in price of goods in the stages before the retail level. Wholesale price indexes (WPIs) report monthly to show the average price changes of goods sold in bulk, and they are a group of the indicators that follow growth in the economy.\par
Although some countries still use the WPIs as a measure of inflation, many countries, including the United States, use the producer price index (PPI) instead.\par

\subsection*{Price sequences}

\subsection*{Other sources}
distribution \& production

exchange rate
crude oil


\section*{Social Media data}
Twitter

\subsection*{Historical tweets}

\subsubsection*{Approach 1: Fetching "historical" tweets via user graph}

Using the Twython package for python we are able to interface with the Twitter API. Our methodology will be to select the twitter accounts of a number of regional celebrities as root nodes. These are likely to 'followed' by large numbers of local users. In a first phase, from each of these roots we may extract a list of followers and filter by various characteristics. Once a substantial list has been constructed, we may proceed to download the tweet activity (up to 3200 tweets) of each of these users in a second phase.

Despite recent updates allowing developers greater access, Twitter still imposes troublesome constraints on the number of requests per unit time window (15 minutes) and, consequently, the data collection rate. It is therefore necessary to: 1) optimise the use of each request; and 2) parallelise the data collection effort.

As far as optimisation is concerned, the \textbf{GET statuses/user\_timeline} call may be called 300 times per 15 minute time window with up to 200 tweets returned per request. This sets a hard upper bound of 60000 tweets per time window. This is why the filtering stage of the first phase is so crucial. Using the \textbf{GET followers/list} call (30 calls/time window), we may discard in advance the majority of twitter users with low numbers of tweets (often zero), so as to avoid burning the limited user timeline requests on fruitless users, thus increasing the data collection rate. With this approach we may approach optimality and achieve 4-5 million tweets daily per process. However, it may be prudent to strike a balance between collection and diversity of account. Therefore a nominal filter is currently set to 50 tweets minimum rather than 200. It is furthermore necessary to install dynamic time-tracking mechanisms within the source code so as to monitor the request rates and to impose a process 'sleep'

Parallelisation will begin with obtaining N ($\approx 10$) sets of developer credentials from Twitter (https://dev.twitter.com/). These N credentials may then be used to launch N processes collecting tweet data in parallel. Given the decision to divide the follower collection and tweet collection into two separate phases (this may alternatively be done simultaneously), there is no need for distributed interaction between the processes to control overlap, as each process will simply take $1/N$ th of the follower list produced in phase 1 and process it accordingly. It should be relatively simple to initiate this parallel computation given the design of the scripts.

\subsubsection*{Approach 2: Filtering tweets provided by webarchive.org}
https://archive.org/details/twitterstream

\subsection*{Daily? tweet aggregator}


\subsection*{Issue of localization}

\subsubsection*{Geolocalized tweets}

\subsubsection*{Approximation: Mapping tweets to user location}


\section*{Processing}

\subsection*{Merging Series}

\subsection*{Crafting indicators from tweets}