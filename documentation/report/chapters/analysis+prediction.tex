\section*{Price Transmission Analysis}

\subsection*{Interpretation}
automate interpretation to a certain extent by learning about circumstances through online data.


\subsection*{Time Series Analysis}
Time series data has a natural temporal relation between different data points.
It is important in the analysis to extract significant temporal statistics out
of data. We will focus on analyze stationarity, autocorrelation, trend, volatility
change, and seasonality of our price datasets in R.

Stationarity of a series guarantees that the mean and variance of the data do not
change over time. This is crucial for a meaningful analysis, since if the data is
not stationary, we can not be sure that anything we derive from the present will
be consistent in the future. We can transform our data into a stationary one by
taking k-th difference to remove the underlying trend, and then apply standard
test procedures such as KPSS test [1] to see if the differenced series is stationary.

Autocorrelation is another important trait in time series data. It suggests the
degree of correlation between different time periods. By plotting correlograms
(autocorrelation plots) of our data, we will be able to identify if the fluctuation
of prices may be due to white noise or other hidden structures.

Seasonality is reasonably expected in our agricultural related time series. Several
methods might help us to detect seasonality, such as common run charts, seasonal
subseries plots, periodograms, and the correolograms we mentioned before.

(trend and volatility change is straightforward and can be concluded once we have the datasets)

[1] Kwiatkowski, D.; Phillips, P. C. B.; Schmidt, P.; Shin, Y. (1992). "Testing the null hypothesis of stationarity against the alternative of a unit root". Journal of Econometrics 54 (1–3): 159–178.


\section*{Prediction Models}

\subsection*{Time Series Forecasting}

\subsubsection*{ARMA Model}
The classical Time series forecasting approach is to use the ARMA (Auto-Regressive
Moving Average) model to predict the target variable as a linear function which
consists of the auto-regressive part (lag variables) and the moving average part
(effects from recent random shocks).

The ARMA(p,q) model: (will refine math representations later)

$Phi(B) * Y_t = Theta(B) * eps_t$

The fitting of the model and the historical
data can be accomplished by maximum likelihood estimation.

\subsubsection*{Regression}
We can also apply ARMA to the linear regression model. It is formulated as such:

$Y = Beta*X + eps,   eps ~ ARMA(p,q)$

Through OLS (Ordinary Leasat Square) or GLS (General Least Square) processes,
we can obtain an optimal Beta.

\subsection*{Multilayer Perceptrons}
taken from M. Seegers course on Pattern Recognition and ML

\subsection*{Recurrent Neural Networks (RNN)}
source: scholarpedia
A recurrent neural network (RNN) is a special network in which neurons send feedback signals to each other. %We have to distinguish between artificial neural networks (aRNN) and biological recurrent neural networks (bRNN) that are responsible for most of the brains dynamics. 
The key feature is that the weight matrix for each layer $l$ in the network contains input weights from \emph{all} other neurons in the network and not just the neurons from the previous layer. 

\subsubsection*{Simple Recurrent Networks}
These links enable signals to be fed back from a layer to a previous layer. The most simple form of an aRNN consists of an input, an output and one hidden layer. 

\includegraphics{../simpe_rnn.svg}

In a \emph{fully recurrent network} every neuron receives input from all other neurons and therefore is not easily arranged into layers. A small subset of neurons additionally receives the external input and another subset produces the output. 

A \emph{Hopfield network} is an RNN all connections of which are symmetric and which requires stationary inputs. 

\subsection*{Echo State Networks (ESN)}
Echo State Networks are a type of Recurrent Neural Network (RNN) applicable to many domains because unlike other RNNs they are easy to train. They have a sparsely connected random hidden layer with the effect that the weights of the output neurons are the only part of the network that can change and be trained. They are good at reproducing time series. (wikipedia)

This property allows for the rest of the network to be topologically unrestricted.

\subsubsection*{Finding the right topology for the specific prediction problem}

\subsubsection*{Converge}
conceive network that converges fast to speed up training\par
"Known supervised training techniques for RNNS comprise Back Propagation Through Time (BPTT), Real Time Recurrent Learn- ing (RTRL) or Extended Kalman Filtering (EKF) all of which have some major drawbacks."
{\em Application of BPTT to RNNs requires stacking identical copies of the network thus unfolding the cyclic paths in the synaptic connections. Unlike back-propagation used in feed-forward nets, BPTT is not guaranteed to con- verge to a local error minimum, computational cost is O(TN2) per time step where N is the number of nodes, T the number of epochs. In contrast RTRL needs O((N + L)4) (L denotes number of output units), which makes this algorithm only applicable for small nets. The algorithm complexity of EKF is O(LN2). EKF is mathematically very elaborate and only a few experts have trained predefined dynamical system behaviors successfully}\par


\subsubsection*{Avoiding overfitting}

\cite{jaeger_echo_state_RNN}
The third section explains how echo state networks can be trained in a
supervised way. The natural approach here is to adapt only the weights
of network-to-output connections. Essentially, this trains readout functions
which transform the echo state into the desired output signal. Technically,
this amounts to a linear regression task.

\subsubsection*{Echo States}
For our task we need a discrete time neural network which is incidentally also the constraint in which Echo State Networks are defined. The ESN is assumed to have $N$ input units, $K$ internal network units and $L$ output units. Direct connections from input to output units and from output to output units are allowed.

\subsubsection*{Training ESN}
